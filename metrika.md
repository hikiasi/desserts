ЯНДЕКС МЕТРИКА:
Способ установки кода счетчика
В коде сайта инициализируйте отправку просмотров тех страниц сайта, на которых требуется собирать статистику. Как это сделать 
<!-- Yandex.Metrika counter -->
<script type="text/javascript">
    (function(m,e,t,r,i,k,a){
        m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
        m[i].l=1*new Date();
        for (var j = 0; j < document.scripts.length; j++) {if (document.scripts[j].src === r) { return; }}
        k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)
    })(window, document,'script','https://mc.yandex.ru/metrika/tag.js?id=106838565', 'ym');

    ym(106838565, 'init', {ssr:true, webvisor:true, clickmap:true, ecommerce:"dataLayer", referrer: document.referrer, url: location.href, accurateTrackBounce:true, trackLinks:true});
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/106838565" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
<!-- /Yandex.Metrika counter -->

1. Добавьте метатег в код сайта В <head> главной страницы сайта вставьте метатег из поля ниже и сохраните.
<meta name="yandex-verification" content="3cd204cc54a0710d" />

Индексирование сайта
Добавьте сайт в Яндекс Вебмастер.

Sitemap. Для удобства вебмастеров и поисковых систем был разработан специальный формат карты сайта — sitemap. Это список ссылок на внутренние страницы сайта, представляемый в формате XML. Этот формат поддерживает и Яндекс. На специальной странице сервиса можно загрузить Sitemap для вашего сайта. Это позволит влиять на приоритет обхода роботом некоторых страниц вашего сайта. Например, если какие-то страницы обновляются гораздо чаще, чем другие, следует указать эту информацию, чтобы робот Яндекса правильно планировал свою работу.

Robots.txt — файл, предназначенный для роботов поисковых систем. В этом файле вебмастер может указать параметры индексирования своего сайта как для всех роботов сразу, так и для каждой поисковой системы по отдельности. Рассмотрим наиболее важные параметры, которые можно указать в этом файле:

Disallow

Эта директива используется для запрета от индексирования отдельных разделов сайта. С ее помощью необходимо закрывать от индексирования технические и не представляющие ценности ни для пользователя, ни для поисковых систем страницы. К ним относятся:

Подробнее об этом см. в разделе Использование файла robots.txt.

Clean param

С помощью этой директивы можно указать роботу, какие cgi-параметры в адресе страницы следует считать незначащими. Иногда адреса страниц содержат идентификаторы сессий — формально страницы с разными идентификаторами различаются, однако их содержимое при этом одинаково. Если таких страниц на сайте много, индексирующий робот может начать индексировать такие страницы, вместо того чтобы скачивать полезное содержимое. Подробнее об этом см. в разделе Использование файла robots.txt.
Яндекс индексирует основные типы документов, распространенных в Сети. Но существуют ограничения, от которых зависит, как будет проиндексирован документ, и будет ли проиндексирован вообще:

Большое количество cgi-параметров в URL, большое количество повторяющихся вложенных директорий и слишком большая общая длина URL может привести к ухудшению индексирования документов.

Для индексирования важен размер документа — документы больше 10Мб не индексируются.

Индексирование flash:

индексируются файлы *.swf, если на них есть прямая ссылка или они встроены в html тегами object или embed ;

если flash содержит полезный контент, исходный html документ может быть найден по контенту, проиндексированному в SWF-файле.

В документах PDF индексируется только текстовое содержимое. Текст, представленный в виде картинок, не индексируется.

Яндекс корректно индексирует документы в формате Open Office XML и OpenDocument (в частности, документы Microsoft Office и Open Office). Но следует учитывать, что внедрение поддержки новых форматов может занимать некоторое время.

Допустимо использование тегов <frameset> и <frame>, робот Яндекса индексирует контент, подгружаемый в них, и позволяет найти исходный документ по содержимому фреймов.

Если вы переопределили поведение сервера для несуществующих URL, убедитесь, что сервер возвращает код ошибки 404. Получив код ответа 404, поисковая система удалит данный документ из индекса. Следите, чтобы все нужные страницы сайта отдавали код 200 OK.

Следите за корректностью HTTP-заголовков. В частности, важно содержание ответа, который сервер отдает на запрос «if-modified-since». Заголовок Last-Modified должен отдавать корректную дату последнего изменения документа.

Примечание

Запрещайте для индексирования не предназначенные для пользователей страницы, управляйте поисковым роботом Яндекса.

Индексирование сайтаSitemap




Использование файла Sitemap
Sitemap — это файл со ссылками на страницы сайта, который сообщает поисковым системам об актуальной структуре сайта. Яндекс поддерживает форматы XML и TXT. Формат XML позволяет передавать дополнительную информацию.

Примечание

Яндекс не гарантирует, что все указанные в файле URL попадут в результаты поиска.

В каких случаях нужен файл Sitemap
Яндекс разрабатывает специальные алгоритмы, по которым индексирующий робот узнает о сайте. Например, с помощью внутренних и внешних ссылок — переходя с одной страницы на другую. Иногда робот может пропустить страницы. Используйте Sitemap, если на сайте:

большое количество страниц;
отдельные страницы без навигационных ссылок;
глубокая вложенность.
Поддерживаемые Яндексом форматы
XML (рекомендуемый)
TXT
Яндекс поддерживает протокол Sitemap. Чтобы передать информацию, используйте следующие элементы:

Тег

Обязательно

Описание

loc

Да

Адрес страницы.

lastmod

Нет

Дата последнего обновления страницы.
Максимальный размер — 100 байтов.

changefreq

Нет

Частота изменения страницы.

Максимальный размер — 100 байтов.

priority

Нет

Значимость страницы. Робот загружает страницы поочередно с учетом наличия и значения коэффициента от 0.0 до 1.0. Укажите коэффициент для тех URL, которые наиболее важны для сайта.

Максимальный размер — 100 байтов.

Пример:

<?xml version="1.0" encoding="UTF-8"?>
    <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"> 
        <url>
            <loc>http://www.example.com/page1.html</loc>
            <lastmod>2005-01-01</lastmod>
            <changefreq>monthly</changefreq>
            <priority>0.8</priority>
        </url>
        ...
    </urlset>



Также с помощью Sitemap вы можете передать информацию о локализованных страницах сайта и об изображениях (см. раздел Справки Яндекс Картинок).

Чтобы передавать в файле Sitemap информацию о видео, используйте микроразметку и добавьте страницы с видео в Sitemap.

Внимание

Поиск Яндекса не поддерживает передачу RSS и Atom-фидов с помощью файла Sitemap.

Требования к файлу
Используйте кодировку UTF-8. Кириллические URL робот Яндекса распознает как в закодированном виде, так и в оригинале.
Максимальное количество ссылок — 50 000. Вы можете разделить Sitemap на несколько отдельных файлов и указать их в файле индекса Sitemap.
Максимальный размер файла в несжатом виде — 50 МБ.
Указывайте ссылки на страницы только того домена, на котором будет расположен файл.
Разместите файл на том же домене, что и сайт, для которого он составлен.
При обращении к файлу сервер должен возвращать HTTP-код 200.
Как создать файл и сообщить о нем Яндексу
Определите канонические URL для страниц, которые будут указаны в файле Sitemap. Это необходимо, если на сайте есть несколько страниц со схожей информацией.
Выберите формат и с помощью одного из сторонних ресурсов сформируйте файл.
Проверьте файл с помощью инструмента Анализ файлов Sitemap.
Укажите ссылку на файл в robots.txt или в разделе Файлы Sitemap Яндекс Вебмастера.
При необходимости вы можете удалить информацию о Sitemap.

Вопросы и ответы
Как организовать Sitemap для большого сайта
Как правило, в Sitemap принято включать все значимые страницы сайта. Если на сайте множество страниц, вы можете удалить из Sitemap уже известные Яндексу страницы и вносить в файл только новые или часто обновляющиеся.

Если вы используете файл индекса Sitemap отметьте тегом lastmod файлы Sitemap, которые часто обновляются.

Определить, какие страницы сайта уже известны Яндексу, можно в Вебмастере на странице Индексирование → Статистика обхода.

Sitemap отображается в сервисе как исключенная страница со статусом «Неверный формат документа»
Отображение файла Sitemap и других XML-файлов как исключенных страниц носит только информативный характер и никак не влияет на индексирование сайта или обработку Sitemap.

Файл Sitemap может отображаться в группе исключенных страниц, так как робот попытался проиндексировать его как обычную страницу. Не смотря на это файлы Sitemap не участвуют в поиске.

При обработке Sitemap возникла ошибка «Неизвестный тег»
Файл Sitemap может содержать только определенные XML-элементы. Если Вебмастер обнаружит в файле другие элементы, например, указание на мобильную версию, в Вебмастере появится ошибка «Неизвестный тег». Неподдерживаемые элементы игнорируются роботом при обработке Sitemap, при этом данные из поддерживаемых элементов учитываются. Поэтому менять файл Sitemap необязательно.

Если содержимое файла будет изменено, потребуется до двух недель на обновление информации в Вебмастере.

Файл Sitemap находится в статусе «Не проиндексирован»
Файл Sitemap может быть не проиндексирован по нескольким причинам:

Робот обошел файл Sitemap недавно и еще не успел обработать его. Подождите две недели. Если вы используете файл индекса Sitemap с несколькими файлами, то их обработка может занять больше времени, чем одного файла Sitemap.
Сайт ранее был недоступен для индексирующего робота. Необходимо дождаться следующего посещения сайта роботом.
Доступ к Sitemap запрещен в файле robots.txt, при обращении к Sitemap сервер возвращает HTTP-код, отличный от 200. Проверьте доступность файла для робота с помощью инструмента Проверка ответа сервера, указав полный путь к файлу.
Если проблема с доступом актуальна, обратитесь к администратору сайта или сервера, на котором он расположен. Если файл составлен корректно, он будет повторно обработан в течение двух недель с момента последнего запроса роботом. Проверить файл можно с помощью инструмента Анализ файлов Sitemap.

Файл Sitemap находится в статусе «Документ не содержит текст»
Такой статус отображается в инструменте Проверка ответа сервера и сообщает, что робот не может проиндексировать файл как обычную страницу сайта и включить его содержимое в результаты поиска. На обработку Sitemap этот статус не влияет. Чтобы проверить корректность Sitemap используйте в Вебмастере инструмент Анализ файлов Sitemap.

Файл Sitemap находится в статусе «URL запрещён к индексированию тегом noindex»
Такой статус отображается в инструменте Проверка ответа сервера и сообщает, что файл закрыт от индексирования — как правило, запрет прописан в HTTP-заголовке X-Robots-Tag. Директива запрещает файлу появляться в результатах поиска, но не влияет на обработку содержимого Sitemap. Чтобы проверить корректность Sitemap используйте в Вебмастере инструмент Анализ файлов Sitemap.

Sitemap не обрабатывается
Как правило, файл Sitemap обрабатывается роботом в течение двух недель после добавления его в Вебмастер. Если этого не произошло, проверьте:

Что Sitemap относится именно к тому сайту, для которого вы добавляете его в Вебмастер. Убедитесь, что адрес сайта совпадает: протокол (HTTP/HTTPS), наличие или отсутствие префикса www.



Ответ сервера при запросе Sitemap. Если доступ к Sitemap запрещен в файле robots.txt или при обращении к нему возвращается HTTP-код, отличный от 200, файл не будет обработан роботом. Если при проверке отображается статус «Запрещен метатегом noindex», это не говорит об ошибке. Метатег noindex запрещает включение файла в поисковую базу, но не мешает роботу обрабатывать его содержимое.

Корректность файла с помощью инструмента Анализ файлов Sitemap. Устраните ошибки, если они обнаружены.

Sitemap составлен корректно и доступен для робота, но не обрабатывается больше двух недель
Какое в Sitemap максимальное число символов для одного URL
Общий лимит символов 2048. Из них 1024 символа — на доменное имя сайта, например https://site.com. Оставшиеся 1024 символа — на путь к внутренним страницам сайта, то есть для символов, которые идут после доменного имени, например /page_1/page_2/....
Использование файла robots.txt
Robots.txt — это текстовый файл, который содержит параметры индексирования сайта для роботов поисковых систем. В robots.txt можно ограничить индексирование роботами страниц сайта, что может снизить нагрузку на сайт и ускорить его работу.

Примечание

Ограниченные в robots.txt страницы могут участвовать в поиске Яндекса. Чтобы удалить страницы из поиска, укажите директиву noindex в HTML-коде страницы или настройте HTTP-заголовок. Не ограничивайте такие страницы в robots.txt, чтобы робот Яндекса смог их проиндексировать и обнаружить ваши указания. Подробно см. в разделе Как удалить страницы из поиска.

Яндекс поддерживает стандарт исключений для роботов (Robots Exclusion Protocol) с расширенными возможностями.


Урок Как управлять индексированием сайта.

Требования к файлу robots.txt
Роботы Яндекса корректно обрабатывают robots.txt, если:

Размер файла не превышает 500 КБ.

Это TXT-файл с названием robots — robots.txt.

Файл размещен в корневом каталоге сайта.

Файл доступен для роботов:

Сервер, на котором размещен сайт, отвечает HTTP-кодом состояния 200 OK.
Файл robots.txt перенаправляет на другой robots.txt, для которого сервер возвращает код 200 OK.
Подробно см. Проверка ответа сервера.

Яндекс поддерживает редирект с файла robots.txt, расположенного на одном сайте, на файл, который расположен на другом сайте. В этом случае учитываются директивы в файле, на который происходит перенаправление. Такой редирект может быть удобен при переезде сайта.

Если файл не соответствует требованиям, сайт считается открытым для индексирования.

Рекомендации по наполнению файла
Яндекс поддерживает следующие директивы:

Директива

Что делает

User‑agent *

Указывает на робота, для которого действуют перечисленные в robots.txt правила.

Disallow

Запрещает обход разделов или отдельных страниц сайта.

Sitemap

Указывает путь к файлу Sitemap, который размещен на сайте.

Clean-param

Указывает роботу, что URL страницы содержит параметры (например, UTM-метки), которые не нужно учитывать при индексировании.

Allow

Разрешает индексирование разделов или отдельных страниц сайта.

Crawl-delay

Задает роботу минимальный период времени (в секундах) между окончанием загрузки одной страницы и началом загрузки следующей.

Рекомендуем вместо директивы использовать настройку скорости обхода в Яндекс Вебмастере.

* Обязательная директива.

Наиболее часто вам могут понадобиться директивы Disallow, Sitemap и Clean-param. Например:

User-agent: * #указывает, для каких роботов установлены директивы
Disallow: /bin/ # запрещает ссылки из "Корзины с товарами".
Disallow: /search/ # запрещает ссылки страниц встроенного на сайте поиска
Disallow: /admin/ # запрещает ссылки из панели администратора
Sitemap: http://example.com/sitemap # указывает роботу на файл Sitemap для сайта
Clean-param: ref /some_dir/get_book.pl

Роботы других поисковых систем и сервисов могут иначе интерпретировать директивы.

Примечание

Робот учитывает регистр в написании подстрок (имя или путь до файла, имя робота) и не учитывает регистр в названиях директив.

Использование кириллицы
Использование кириллицы запрещено в файле robots.txt и HTTP-заголовках сервера.

Для указания имен доменов используйте Punycode. Адреса страниц указывайте в кодировке, соответствующей кодировке текущей структуры сайта.

Пример файла robots.txt:

Неверно:

User-agent: Yandex
Disallow: /корзина
Sitemap: сайт.рф/sitemap.xml

Верно:

User-agent: Yandex
Disallow: /%D0%BA%D0%BE%D1%80%D0%B7%D0%B8%D0%BD%D0%B0
Sitemap: http://xn--80aswg.xn--p1ai/sitemap.xml

Как создать robots.txt
В текстовом редакторе создайте файл с именем robots.txt и укажите в нем нужные вам директивы.
Проверьте файл в Вебмастере.
Положите файл в корневую директорию вашего сайта.
Пример файла. Данный файл разрешает индексирование всего сайта для всех поисковых систем
Директива User-agent
В файле robots.txt робот проверяет наличие записей, начинающихся с User-agent:, в них учитываются подстроки Yandex (регистр значения не имеет) или *. Если обнаружена строка User-agent: Yandex, то строка User-agent: * не учитывается. Если строки User-agent: Yandex и User-agent: * отсутствуют, считается, что доступ роботу не ограничен.

Роботам Яндекса можно указать отдельные директивы. О том, как роботы учитывают их, см. в разделе Роботы Яндекса в логах сервера.

Если обнаружены директивы для конкретного робота, директивы User-agent: Yandex и User-agent: * не используются.

Пример:

User-agent: YandexBot # будет использоваться только основным индексирующим роботом
Disallow: /*id=

User-agent: Yandex # будет использована всеми роботами Яндекса
Disallow: /*sid= # кроме основного индексирующего

User-agent: * # не будет использована роботами Яндекса
Disallow: /cgi-bin 

В соответствии со стандартом перед каждой директивой User-agent рекомендуется вставлять пустой перевод строки. Символ # предназначен для описания комментариев. Все, что находится после этого символа и до первого перевода строки не учитывается. Подробно см. в разделе Обработка символа #.

Примечание

Роботы Яндекса могут выделять записи по наличию в строке User-agent:.
Директивы Disallow и Allow
Disallow
Используйте эту директиву, чтобы запретить обход разделов сайта или отдельных страниц. Например:

страницы с конфиденциальными данными;
страницы с результатами поиска по сайту;
статистика посещаемости сайта;
дубликаты страниц;
разнообразные логи;
сервисные страницы баз данных.
Примечание

При выборе директивы для страниц, которые не должны участвовать в поиске, если их адреса содержат GET-параметры, лучше использовать директиву Clean-param, а не Disallow. При использовании Disallow может не получиться выявить дублирование адреса ссылки без параметра и передать некоторые показатели запрещенных страниц.

Примеры:

User-agent: Yandex
Disallow: / # запрещает обход всего сайта

User-agent: Yandex
Disallow: /catalogue # запрещает обход страниц, адрес которых начинается с /catalogue

User-agent: Yandex
Disallow: /page? # запрещает обход страниц, URL которых содержит параметры

Allow
Директива разрешает обход разделов или отдельных страниц сайта.

Примеры:

User-agent: Yandex
Allow: /cgi-bin
Disallow: /
# запрещает скачивать все, кроме страниц 
# начинающихся с '/cgi-bin'

User-agent: Yandex
Allow: /file.xml
# разрешает скачивание файла file.xml

Примечание

Недопустимо наличие пустых переводов строки между директивами User-agent, Disallow и Allow.

Совместное использование директив
Директивы Allow и Disallow из соответствующего User-agent блока сортируются по длине префикса URL (от меньшего к большему) и применяются последовательно. Если для данной страницы сайта подходит несколько директив, то робот выбирает последнюю в порядке появления в сортированном списке. Таким образом, порядок следования директив в файле robots.txt не влияет на использование их роботом.

Примечание

При конфликте между двумя директивами с префиксами одинаковой длины приоритет отдается директиве Allow.

# Исходный robots.txt:
User-agent: Yandex
Allow: /
Allow: /catalog/auto
Disallow: /catalog

# Сортированный robots.txt:
User-agent: Yandex
Allow: /
Disallow: /catalog
Allow: /catalog/auto
# запрещает скачивать страницы, начинающиеся с '/catalog',
# но разрешает скачивать страницы, начинающиеся с '/catalog/auto'.

Общий пример:

User-agent: Yandex
Allow: /archive
Disallow: /
# разрешает все, что содержит '/archive', остальное запрещено

User-agent: Yandex
Allow: /obsolete/private/*.html$ # разрешает html файлы
                                 # по пути '/obsolete/private/...'
Disallow: /*.php$  # запрещает все '*.php' на данном сайте
Disallow: /*/private/ # запрещает все подпути содержащие
                      # '/private/', но Allow выше отменяет
                      # часть запрета
Disallow: /*/old/*.zip$ # запрещает все '*.zip' файлы, содержащие 
                        # в пути '/old/'

User-agent: Yandex
Disallow: /add.php?*user= 
# запрещает все скрипты 'add.php?' с параметром 'user'

Директивы Allow и Disallow без параметров
Если директивы не содержат параметры, робот учитывает данные следующим образом:

User-agent: Yandex
Disallow: # то же, что и Allow: /

User-agent: Yandex
Allow: # не учитывается роботом

Использование спецсимволов * и $
При указании путей директив Allow и Disallow можно использовать спецсимволы * и $, чтобы задавать определенные регулярные выражения.

Спецсимвол * означает любую (в том числе пустую) последовательность символов. Примеры:

User-agent: Yandex
Disallow: /cgi-bin/*.aspx # запрещает '/cgi-bin/example.aspx'
                          # и '/cgi-bin/private/test.aspx'
Disallow: /*private # запрещает не только '/private',
                    # но и '/cgi-bin/private'

По умолчанию к концу каждого правила, описанного в файле robots.txt, приписывается спецсимвол *. Пример:

User-agent: Yandex
Disallow: /cgi-bin* # блокирует доступ к страницам 
                    # начинающимся с '/cgi-bin'
Disallow: /cgi-bin # то же самое

Чтобы отменить * на конце правила, можно использовать спецсимвол $, например:

User-agent: Yandex
Disallow: /example$ # запрещает '/example', 
                    # но не запрещает '/example.html'

User-agent: Yandex
Disallow: /example # запрещает и '/example', 
                   # и '/example.html'

Спецсимвол $ не запрещает указанный * на конце, то есть:

User-agent: Yandex
Disallow: /example$  # запрещает только '/example'
Disallow: /example*$ # так же, как 'Disallow: /example' 
                     # запрещает и /example.html и /example

Обработка символа #
В соответствии со стандартом перед каждой директивой User-agent рекомендуется вставлять пустой перевод строки. Символ # предназначен для описания комментариев. Все, что находится после этого символа и до первого перевода строки не учитывается.

Страницы с адресами вида https://example.com/page#part_1 не индексируются поисковым роботом и будут обходиться по адресу https://example.com/page. Поэтому в директиве достаточно указать адрес страницы без якоря.

Если не учесть этой особенности и написать запрещающую директиву с символом #, она может закрыть от индексирования весь сайт. Например, директиву вида Disallow: /# поисковая система воспримет как Disallow: / — полный запрет на индексирование.

Примеры интерпретации директив
User-agent: Yandex 
Allow: /
Disallow: /
# все разрешается

User-agent: Yandex 
Allow: /$
Disallow: /
# запрещено все, кроме главной страницы

User-agent: Yandex
Disallow: /private*html
# запрещается и '/private*html', 
# и '/private/test.html', и '/private/html/test.aspx' и т. п.

User-agent: Yandex
Disallow: /private$
# запрещается только '/private'

User-agent: *
Disallow: /
User-agent: Yandex
Allow: /
# так как робот Яндекса 
# выделяет записи по наличию в строке 'User-agent:', 
# результат — все разрешается
Директива Sitemap
Если вы используете описание структуры сайта с помощью файла Sitemap, укажите путь к файлу в качестве параметра директивы Sitemap (если файлов несколько, укажите все). Пример:

User-agent: Yandex
Allow: /
sitemap: https://example.com/site_structure/my_sitemaps1.xml
sitemap: https://example.com/site_structure/my_sitemaps2.xml

Директива является межсекционной, поэтому будет использоваться роботом вне зависимости от места в файле robots.txt, где она указана.

Робот запомнит путь к файлу, обработает данные и будет использовать результаты при последующем формировании сессий загрузки.
Директива Clean-param
Примечание

Иногда для закрытия таких страниц используется директива Disallow. Рекомендуем использовать Clean-param, так как эта директива позволяет передавать основному URL или сайту некоторые накопленные показатели.

Обучающее видео

Как использовать директиву Clean-param.

Посмотреть видео
Как использовать директиву Clean-param
Заполняйте директиву Clean-param максимально полно и поддерживайте ее актуальность. Новый параметр, не влияющий на контент страницы, может привести к появлению страниц-дублей, которые не должны попасть в поиск. Из-за большого количества таких страниц робот медленнее обходит сайт. А значит, важные изменения дольше не попадут в результаты поиска.

Робот Яндекса, используя эту директиву, не будет многократно перезагружать дублирующуюся информацию. Таким образом, увеличится эффективность обхода вашего сайта, снизится нагрузка на сервер.

Например, на сайте есть страницы:

www.example.com/some_dir/get_book.pl?ref=site_1&book_id=123
www.example.com/some_dir/get_book.pl?ref=site_2&book_id=123
www.example.com/some_dir/get_book.pl?ref=site_3&book_id=123

Параметр ref используется только для того, чтобы отследить с какого ресурса был сделан запрос и не меняет содержимое, по всем трем адресам будет показана одна и та же страница с книгой book_id=123. Тогда, если указать директиву следующим образом:

User-agent: Yandex
Clean-param: ref /some_dir/get_book.pl

Робот Яндекса сведет все адреса страницы к одному:

www.example.com/some_dir/get_book.pl?book_id=123

Если на сайте доступна такая страница, именно она будет участвовать в результатах поиска.

Чтобы директива применялась к параметрам на страницах по любому адресу, не указывайте адрес:

User-agent: Yandex
Clean-param: utm

Совет

Директива Clean-Param является межсекционной, поэтому может быть указана в любом месте файла. Если вы указываете другие директивы именно для робота Яндекса, перечислите все предназначенные для него правила в одной секции. При этом строка User-agent: * будет проигнорирована.

Синтаксис директивы
Clean-param: p0[&p1&p2&..&pn] [path]

В первом поле через символ & перечисляются параметры, которые роботу не нужно учитывать. Во втором поле указывается префикс пути страниц, для которых нужно применить правило.

Префикс может содержать регулярное выражение в формате, аналогичном файлу robots.txt, но с некоторыми ограничениями: можно использовать только символы A-Za-z0-9.-/*_. При этом символ * трактуется так же, как в файле robots.txt: в конец префикса всегда неявно дописывается символ *. Например:

Clean-param: s /forum/showthread.php

означает, что параметр s будет считаться незначащим для всех URL, которые начинаются с /forum/showthread.php. Второе поле указывать необязательно, в этом случае правило будет применяться для всех страниц сайта.

Регистр учитывается. Действует ограничение на длину правила — 500 символов. Например:

Clean-param: abc /forum/showthread.php
Clean-param: sid&sort /forum/*.php
Clean-param: someTrash&otherTrash

Параметры, которые удаляются автоматически
Параметры аналитики и отслеживания, которые не влияют на содержание страницы, могут автоматически удаляться поисковой системой, если алгоритмы посчитают их незначимыми.

Вебмастер отметит URL с такими параметрами как исключенные директивой Clean-param.

Примечание

Для таких параметров не требуется дополнительных действий в robots.txt — они автоматически не попадут в поиск Яндекса.

Примеры параметров
Дополнительные примеры
#для адресов вида:
www.example1.com/forum/showthread.php?s=681498b9648949605&t=8243
www.example1.com/forum/showthread.php?s=1e71c4427317a117a&t=8243

#robots.txt будет содержать:
User-agent: Yandex
Clean-param: s /forum/showthread.php

#для адресов  вида:
www.example2.com/index.php?page=1&sid=2564126ebdec301c607e5df
www.example2.com/index.php?page=1&sid=974017dcd170d6c4a5d76ae

#robots.txt будет содержать:
User-agent: Yandex
Clean-param: sid /index.php

#если таких параметров несколько:
www.example1.com/forum_old/showthread.php?s=681498605&t=8243&ref=1311
www.example1.com/forum_new/showthread.php?s=1e71c417a&t=8243&ref=9896

#robots.txt будет содержать:
User-agent: Yandex
Clean-param: s&ref /forum*/showthread.php

#если параметр используется в нескольких скриптах:
www.example1.com/forum/showthread.php?s=681498b9648949605&t=8243
www.example1.com/forum/index.php?s=1e71c4427317a117a&t=8243

#robots.txt будет содержать:
User-agent: Yandex
Clean-param: s /forum/index.php
Clean-param: s /forum/showthread.php

Disallow и Clean-param
Директива Clean-param не требует обязательного сочетания с директивой Disallow.

User-agent: Yandex
Disallow:
Clean-param: s&ref /forum*/showthread.php

#идентично:
User-agent: Yandex
Clean-param: s&ref /forum*/showthread.php

Так как директива Clean-param межсекционная, ее можно указывать в любом месте файла, вне зависимости от расположения директив Disallow и Allow. Исполнение Disallow при этом имеет приоритет и, если адрес страницы запрещен к индексированию в Disallow и одновременно ограничен в Clean-param, страница проиндексирована не будет.

User-agent: Yandex
Disallow:/forum
Clean-param: s&ref /forum*/showthread.php

В этом случае страница https://example.com/forum?ref=page будет считаться запрещенной. Не указывайте директиву Disallow для страниц, если хотите только удалить из поиска варианты ссылок с GET-параметрами.

Как добавить сайт в поиск
Страницы сайта могут появиться в результатах поиска, когда роботы Яндекса посетят сайт. Чтобы роботы обошли и загрузили страницы:

Шаг 1. Сделайте страницы сайта видимыми для робота

Чтобы робот Яндекса узнавал об изменениях на сайте, можно использовать несколько способов:

Способ

Автоматизация

Рекомендация

Файл Sitemap


Сформируйте и обновляйте файл. Это позволит передавать информацию о всех URL-адресах сайта. Робот может обрабатывать содержимое файла при очередном обходе сайта.

Обход страниц со счетчиком Яндекс Метрикой


Установите на сайт счетчик Метрики и привяжите его к сайту в Яндекс Вебмастере. Так вы сможете сообщать о популярных страницах.

Переобход страниц

—

В Яндекс Вебмастере на странице Индексирование → Переобход страниц вы можете дать сигнал роботу о том, чтобы он посетил определенные страницы сайта. С помощью этого способа вы можете сообщать об изменениях наиболее важных или новых страниц.

Протокол IndexNow


Помогает автоматически сообщать об изменившихся, новых и удаленных страницах. Требует навыков работы с API.

Совет

Поддерживайте качество сайта. Чем больше полезных для посетителей страниц робот находит и загружает в базу, тем выше вероятность, что они начнут отображаться в результатах поиска. Подробно см. в разделе Признаки некачественного сайта.

Шаг 2. Скройте непубличный контент

Тип страницы

Что делать

Страницы действий. Например, добавление товара в корзину или сравнение товаров.

Запретите индексирование страниц

Корзина с товаром.

Персональная информация. Например, адреса доставки и телефоны ваших клиентов.

Ограничьте доступ к данным с помощью авторизации пользователей на сайте

Страница-дубль. Например, URL с дополнительными параметрами (https://example.com/page?id=1).

Укажите страницу, предпочитаемую для участия в поиске

После того, как робот обойдет сайт, страницы смогут появиться в поиске в течение двух недель.

Кроме того, роботы могут узнавать о сайте, переходя по ссылкам с других ресурсов. Это может занять время и не гарантирует, что робот обойдет все страницы, которые вы бы хотели показать в результатах поиска.

Подробно о том, как работает поиск Яндекса

Чтобы следить за индексированием и позициями сайта в поисковой выдаче, добавьте сайт в Яндекс Вебмастер. Также в Яндекс Вебмастере вы можете посмотреть отображение сайта в поиске и как вы можете улучшить его — на странице Представление в поиске.

Почему страницы долго не появляются в поиске
Убедитесь, что:

страницы доступны для робота (используйте инструмент Проверка ответа сервера);
информация о страницах есть в файле Sitemap;
на страницы ведут доступные ссылки с ранее проиндексированных страниц сайта;
директивы Disallow, noindex, а также HTML-элемент noindex закрывают от индексирования только служебные и дублирующие страницы.
Проверить, что знает робот Яндекса о странице, можно с помощью инструмента Проверить URL. Чтобы сообщить роботу о появлении или обновлении нескольких страниц, отправьте их на переобход.

Поддержка протокола IndexNow
Яндекс поддерживает протокол IndexNow, который позволяет автоматически сообщать поисковым системам об изменениях на сайте: появлении новых страниц, обновлении или удалении уже проиндексированных страниц. С помощью IndexNow вы можете напрямую уведомить Яндекс об изменениях на сайте, не дожидаясь очередного обхода индексирующим роботом.

Чтобы использовать протокол, вам нужно настроить передачу информации о страницах с помощью API с использованием HTTP и JSON для обмена данными.

Для отправки URL-адресов необходимо подтвердить, что именно вы являетесь владельцем сайта, для которого передаются данные. Для подтверждения используется специальный ключ — его нужно сформировать, разместить файл с ним на вашем сайте и передавать в запросах к API. При каждом запросе к API Яндекс проверяет ключ.

Примечание

Данный способ не гарантирует, что переданные страницы будут проиндексированы.

Начало работы
Сформируйте и разместите на сайте ключ.

Отправьте в Яндекс URL новых, изменившихся или удаленных страниц сайта через:

API

Чтобы отправить один адрес, используйте метод GET https://yandex.com/indexnow, для передачи нескольких адресов — POST https://yandex.com/indexnow. Подробнее см. в разделе Справочник запросов API IndexNow.
Плагины для CMS

Посмотрите плагины для распространенных CMS, которые помогают автоматизировать работу с IndexNow.
Вопросы и ответы
Если я только начал использовать IndexNow, нужно ли передавать страницы, изменившиеся в прошлом году?
Нет, передавайте страницы, которые изменились с начала поддержки IndexNow.

Можно ли отправить URL всех страниц сайта?
Сообщайте только о новых, изменившихся или удаленных страницах. Чтобы Яндекс регулярно узнавал о всех изменениях на вашем сайте, используйте другие способы. Например, обход страниц со счетчиком Метрики или файл Sitemap.

Мой сайт небольшой, с несколькими страницами. Нужно ли использовать IndexNow?
Вы можете использовать Переобход страниц в Вебмастере.
